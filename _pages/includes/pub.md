
# üìù Publications 
(<sup>*</sup> indicates equal contribution;  <sup>#</sup> indicates corresponding authorship.) 


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">SIGIR 2023</div><img src='images/RPC.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Learn from Relational Correlations and Periodic Events for Temporal Knowledge Graph Reasoning](https://liangke23.github.io/) \\
Ke Liang, <b>Lingyuan Meng</b>, Meng Liu, Yue Liu, Wenxuan Tu, Siwei Wang, Sihang Zhou, Xinwang Liu<sup>#</sup>. (ACM SIGIR 2023) 

- In this paper, by rethinking the collaborative relationship between the generator and the substitute model, we design a novel black-box attack framework. The proposed method can efficiently imitate the target model through a small number of queries and achieve high attack success rate.

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Under Review</div><img src='images/SARF.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[SARF: Aliasing Relation Assisted Self-Supervised Learning for Few-shot Relation Reasoning](https://arxiv.org/pdf/2304.10297.pdf) \\
<b>Lingyuan Meng<sup>*</sup></b>, Ke Liang<sup>*</sup>, Bin Xiao, Sihang Zhou, Yue Liu, Meng Liu, Xihong Yang, Xinwang Liu<sup>#</sup>. (Under Review) 

- In this work, we investigate the label distribution skew from a statistical view. We demonstrate both theoretically and empirically that previous methods based on softmax crossentropy are not suitable, which can result in local models heavily overfitting to minority classes and missing classes. Then, we propose FedLC (Federated learning via Logits
Calibration), which calibrates the logits before softmax cross-entropy according to the probability of occurrence of each class.

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Under Review</div><img src='images/AKGR.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
[Reasoning over Different Types of Knowledge Graphs: Static, Temporal and Multi-Modal](https://arxiv.org/pdf/2212.05767.pdf) \\
  Ke Liang, <b>Lingyuan Meng</b>, Meng Liu, Yue Liu, Wenxuan Tu, Siwei Wang, Sihang Zhou, Xinwang Liu, Fuchun Sun.
  (Under Review)

- In this paper, we assume that training the synthetic data with diverse models leads to better generalization performance. Thus we propose two model augmentation techniques, i.e., using early-stage models and weight perturbation to learn an informative synthetic set with significantly reduced training cost. Extensive experiments demonstrate that our method achieves up to 20√ó speedup and comparable performance on par with state-of-the-art baseline methods.
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Under Review</div><img src='images/MINES.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Message Intercommunication for Inductive Relation Reasoning](https://arxiv.org/abs/2305.14074) \\
Ke Liang<sup>*</sup>, <b>Lingyuan Meng</b><sup>*</sup>, Sihang Zhou, Siwei Wang, Wenxuan Tu, Yue Liu, Meng Liu, Xinwang Liu. (Under Review) 

- The paper focuses on one-shot federated learning, i.e., the server can learn a model with a single communication round. The proposed FedSyn method has two stages: first, training a generator from the ensemble of models from clients; second, distilling the knowledge of the ensemble into a global model with synthetic data. We validate the efficacy of FedSyn by conducting extensive experiments on 6 different datasets with various non-IID settings generated from Dirichlet distributions. Results can well support that the proposed method consistently outperforms all the baselines.
</div>
</div> -->

